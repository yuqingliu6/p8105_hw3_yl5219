---
title: "p8105_hw3_yl5219"
author: "Yuqing Liu"
date: "2023-10-14"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(ggplot2)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)


scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

### Problem 1

#### read in data 
```{r}
library(p8105.datasets)
data("instacart")

instacart = 
  instacart |> 
  as_tibble()
```

* a short description of the dataset:

  * The dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. 
  * Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. 
  * In total, there are `r instacart |> select(product_id) |> distinct() |> count()` products found in `r instacart |> select(user_id, order_id) |> distinct() |> count()` orders from `r instacart |> select(user_id) |> distinct() |> count()` distinct users.


* How many aisles are there, and which aisles are the most items ordered from?

In total, there are `r instacart |> select(aisle) |> distinct() |> count()`  aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart |> 
  count(aisle) |> 
  arrange(desc(n))
```

* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

* Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()
```

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

### Problem 2

## read in data
```{r}
library(p8105.datasets)
data("brfss_smart2010")

brfss_smart2010
```

First, do some data cleaning:

* format the data to use appropriate variable names by `janitor::clean_names()`;
* focus on the “Overall Health” topic by `filter(topic == "Overall Health")`
* include only responses from “Excellent” to “Poor” by ` filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))`
* organize responses as a factor taking levels ordered from “Poor” to “Excellent” by `factor()` function

```{r}
brfss_smart2010 = brfss_smart2010 |> 
  as_tibble()|>
  janitor::clean_names()|>
  filter(topic == "Overall Health")|> 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))|>
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))

brfss_smart2010
```



Using this dataset, do or answer the following (commenting on the results of each):

* In 2002, which states were observed at 7 or more locations?
  
  * filter only the year 2002 by `filter()`
  * group the data by states by `group_by`
  * find the distinct number of different locations by `summarize(n = n_distinct(locationdesc)`
  * filter only the results with 7 or more locations by `filter(n>=7)`

```{r}
brfss_smart2010|>
  filter(year == 2002)|>
  group_by(locationabbr)|>
  summarize(n = n_distinct(locationdesc))|>
  filter(n>=7)
```

In 2002, CT(Connecticut), FL(Florida), MA(Massachusetts), NC(North Carolina), NJ(New Jersey), PA(Pennsylvania) states were observed at 7 or more locations.

*  In 2010, which states were observed at 7 or more locations?

  * similarly, filter only the year 2010 by `filter()`
  * group the data by states by `group_by`
  * find the distinct number of different locations by `summarize(n = n_distinct(locationdesc)`
  * filter only the results with 7 or more locations by `filter(n>=7)`
  
```{r}
brfss_smart2010|>
  filter(year == 2010)|>
  group_by(locationabbr)|>
  summarize(n = n_distinct(locationdesc))|>
  filter(n>=7)
```
In 2002, CA(California), CO(Colorado), FL(Florida), MA(Massachusetts),	MD(Maryland), NC(North Carolina), NE(Nevada), NJ(New Jersey), NY(New York), OH(Ohio) states were observed at 7 or more locations.

* Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

  * limit to Excellent responses by ` filter(response == "Excellent")`
  * group the data by year and states by ` group_by(year, locationabbr)`
  * create a summary table with a variable that averages the data_value by `summarize(data_value_mean=mean(data_value, na.rm = TRUE))`
  
```{r}
df = brfss_smart2010|>
  filter(response == "Excellent")|>
  group_by(year, locationabbr)|>
  summarize(data_value_mean=mean(data_value, na.rm = TRUE ), .groups = "drop")

df
```

* Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

  * create a plot of mean data value over the year among different states by ` ggplot(aes(x = year, y = data_value_mean, group = locationabbr, color = locationabbr))`
  * add `geom_line()` to create spaghetti-like lines
  * add `geom_point()` to add little dots

```{r}
df |> 
  ggplot(aes(x = year, y = data_value_mean, group = locationabbr, color = locationabbr)) +
  geom_line() + geom_point(size = 0.5)
```

**Comment:**

The average data_value for an excellent response varies between 2002 and 2010. In most states, the average data_value for an excellent response falls within the range of approximately 17 to 27.

<br>

* Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

  * filter only the New York State by `filter(locationabbr=="NY"`
  * filter only the year in 2006 and 2010 by `filter(locationabbr=="NY"`
  * make boxplots showing distribution of data_value for responses by `ggplot( aes(x = response, y = data_value)) + geom_boxplot()`
  * make a two-panel plot by two years through `facet_wrap(~year, ncol = 2)` 

```{r}
# Create a two-panel plot
brfss_smart2010 |>
  filter(locationabbr=="NY")|>
  filter(year %in% c(2006,2010))|>
  ggplot( aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_wrap(~year, ncol = 2) +
  labs(title = "Distribution of data_value by Response in New York State in 2006 and 2010",
       x = "Response",
       y = "data_value") +
  theme_minimal()
```

**Comment:**

  * In both 2006 and 2010, the median data values are lowest for 'Poor' responses, while 'Good,' 'Very Good,' and 'Excellent' responses consistently exhibit significantly higher data values compared to 'Poor' or 'Fair' responses.

  * Notably, in 2010, 'Fair' responses show increased variation compared to 2002. Conversely, 'Good' responses in 2010 display reduced variation compared to 2002.

  * Moreover, in 2010, 'Very Good' responses have a higher median and greater variation compared to 2002.

### Problem 3

* Load, tidy, merge, and otherwise organize the data sets. Your final dataset should include all originally observed variables; exclude participants less than 21 years of age, and those with missing demographic data; and encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind).

  * load and tidy `nhanes_accel` dataset:
    * clean the variable names by `janitor::clean_names()`
    * convert the dataset into long format by `pivot longer`
    * select only the value of `min` variable by `separate(min, into = c("prefix", "min"), sep = 3)` and `select(-prefix)`
  
```{r}
nhanes_accel = read.csv("./nhanes_accel.csv")|> 
  janitor::clean_names()|>
  pivot_longer(
    min1:min1440,
    names_to = "min",
    values_to = "mims_value"
  )|>
  separate(min, into = c("prefix", "min"), sep = 3)|>
  select(-prefix)

nhanes_accel
```
  * load and tidy `nhanes_covar` dataset:
    * clean the variable names by `janitor::clean_names()`
    * exclude participants less than 21 years of age by `filter(age >= 21)`
    * excluse those with missing demographic data by `drop_na()`
    * encode `education` and  `sex` with reasonable variable classes by mutating `education` and `sex` with corresponding class names by `case_match()`
    
```{r}
nhanes_covar = read.csv("./nhanes_covar.csv", skip = 4)|>
  janitor::clean_names()|>
  as_tibble()|>
  filter(age >= 21)|>
  drop_na()|>
  mutate(education = case_match(education, 
                                1 ~ "less than high school",
                                2 ~ "high school equivalent",
                                3 ~ "more than high school"))|>
  mutate(sex = case_match(sex, 
                                1 ~ "male",
                                2 ~ "female"))

nhanes_covar
```
  * merge `nhanes_accel` and `nhanes_covar` dataset together:
    * merge the datasets together by `inner_join()` to include variables values that exists in both datasets
    * relevel the `education` by `forcats::fct_relevel()` so that it is ordered from lower to higher educational attainment instead of being ordered alphabetically
  
```{r}
nhanes = inner_join(nhanes_accel, nhanes_covar, by = "seqn")|>
  mutate(education = forcats::fct_relevel(education, c("less than high school", "high school equivalent", "more than high school"))) 
nhanes
```
  

* Produce a reader-friendly table for the number of men and women in each education category

```{r}
nhanes|>
  group_by(sex, education)|>
  summarize(n_obs = n_distinct(seqn), .groups = "drop")|>
  pivot_wider(names_from = education, values_from = n_obs)|>
  knitr::kable()

```

**Comment:**

  * Among the total 110 female participants, 23 of them have educational level of high school equivalent, 28 have less than high school and 59 have more than high school.
  * Among the total 118 female participants, 35 of them have educational level of high school equivalent, 27 have less than high school and 56 have more than high school.  
  * Almost half of the participants have educational level of more than high school in both female (59 out of 110) and male (56 out of 118) subgroups.
  * In high school equivalent stratum, there is relatively more male than female (35 vs. 23).
  * The numbers of female and male in each category are similar in less than high school stratum (28 vs. 27) and more than high school stratum (59 vs. 56).

<br>

* create a visualization of the age distributions for men and women in each education category.
  * group the data by `sex` and `education` by `group_by()`
  * find the distinct variables like `seqn`by `distinct()`
  * plot the age distribution by education using `ggplot(aes(x = age, group = education, fill = education))`
  * make two separate plots by `sex` using `facet_grid()`
  
```{r}
nhanes|>
  group_by(sex, education)|>
  distinct(seqn,sex,age,education)|>
  ggplot(aes(x = age, group = education, fill = education)) + 
  labs(title = "Age distributions among female and male") +
  geom_density(alpha = .4) +
  facet_grid(~sex)+ 
  viridis::scale_fill_viridis(discrete = TRUE)
```

**Comment:**

  * The educational level of **more than high school** has a right-skewed shape of age distribution both among female (with a peak at approximately 32) and male (with a peak at approximately 31).
  * The educational level of **less than high school** and **high school equivalent** have a left-skewed shape of age distribution among female, with a peak at approximately 70 and 72 respectively. 
  * The educational level of **less than high school** (peaks at 35 and 71) and **high school equivalent** (peaks at 30 and 59) have a bi-modal shape of age distribution among male. 
  * We can see that the disparity in the distribution of education levels among females is notably more pronounced than that observed in the male group. This suggests that gender may indeed influence educational attainment, with females exhibiting a larger variation in age across different education levels (ie., among female, participants with educational level of more than high school are more likely to be younger, and participants with educational level of less than high school and high school equivalent are more likely to be older).



* Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. 

  * create a variable `total_activity`:
    * group the data by `seqn` `sex` `age` and `education` by `group_by`
    * aggregate activity variable across minutes by `summarise(total_activity=sum(mims_value))`
```{r}
total_activity = nhanes|>
  group_by(seqn,sex,age,education)|>
  summarize(total_activity=sum(mims_value),.groups = "drop")
  
total_activity
```
  * plot total activities (y-axis) against age (x-axis):
    * create a plot of `age` vs. `total_activity` and compare men to women by `ggplot(aes(x = age, y = total_activity, group = sex, color = sex))`
    * add points by `geom_point()`
    * add a smooth by `geom_smooth()`
    * separate panels for each education level by `facet_grid(~education)`
    * add title by `labs(title = "Plot of age vs. total activities by educational level and sex")`
  
```{r warning = FALSE, message = FALSE}
total_activity |>
  ggplot(aes(x = age, y = total_activity, group = sex, color = sex)) + 
  geom_point(alpha= 0.75) +
  geom_smooth() +
  facet_grid(~education) +
  labs(title = " Plot of age vs. total activities by educational level and sex") 
```

**Comment:**

* In general, young females and young males exhibit higher levels of total activity across all education groups.

* Specifically, within the 'High school equivalent' and 'More than high school' education groups, total activity levels are slightly higher for females compared to males.

* However, given the considerable overlap in confidence intervals between females and males across all education groups, there is no statistically significant difference in total activity between the genders within these education categories.

* Additionally, both females and males within the 'More than high school' education category display less variability in total activity across age groups, in contrast to the other two education categories.

<br>

* Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based:

    * group the data by `group_by`
    * create a summary data frame with the mean MIMS values by `summarize(mims_value = mean(mims_value))`
    * create a plot of `mims_value` activity across 24-hour `min` and compare men to women by `ggplot(aes(x = min, y = mims_value, group = sex, color = sex))`
    * add points by `geom_point()`
    * add a smooth by `geom_smooth()`
    * separate panels for each education level by `facet_grid(~education)`
    
```{r warning = FALSE, message = FALSE}
nhanes |>
  group_by(min, sex, education)|>
  summarize(mims_value = mean(mims_value), .groups = "drop")|>
  ggplot(aes(x = as.numeric(min), y = mims_value, group = sex, color = sex)) +
  geom_point(alpha =0.75, size = 1.5) + geom_smooth()+
  facet_grid(~education)+
  scale_x_continuous(breaks = seq(0, 1500, by = 200))
```


**Comment**

* Both females and males tend to exhibit higher MIMS values between 8:30 and 20:00, which is also known as between the 510th to the 1200th minute of the day, across all education groups. The MIMS values show a significant increase starting at 4:00 (240th minute), reaching their peak around 8:30, followed by a gradual decrease from 8:30 to 20:00 and a sharp drop after 20:00.

* Within the 'Less than high school' education category, both females and males tend to have higher peak MIMS values compared to the other two education categories. This suggests a potential association between educational attainment and differences in peak MIMS values, with lower education levels potentially correlating with higher peak MIMS scores.

* Female seems to have higher variations in MIMS values across different time as we can see the MIMS score among female is a little bit lower than male before 4:00 while the MIMS score among female is higher than male between 8:30 and 20:00. And later after 20:00 MIMS score among female again becomes a little bit lower than that among male. This trend appear in all three educational groups. Worth noticing, females tend to exhibit much higher MIMS values than males between 8:30 and 20:00 in all three educational groups.
